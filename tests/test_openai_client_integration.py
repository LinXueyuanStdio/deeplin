"""
ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯çš„é›†æˆæµ‹è¯•
"""
import asyncio
import sys
import os
import pytest
import pytest_asyncio
import openai
from httpx import AsyncClient
from fastapi.testclient import TestClient

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# ä½¿ç”¨æ™ºèƒ½æµ‹è¯•åº”ç”¨é…ç½®
from test_app import app

# å›ºå®šçš„ API key
API_KEY = "sk-deeplin-fastapi-proxy-key-12345"

class TestOpenAIClientIntegration:
    """ä½¿ç”¨çœŸå® OpenAI å®¢æˆ·ç«¯çš„é›†æˆæµ‹è¯•"""

    @pytest_asyncio.fixture
    async def openai_client(self):
        """åˆ›å»ºè¿æ¥åˆ°æµ‹è¯•æœåŠ¡å™¨çš„ OpenAI å®¢æˆ·ç«¯"""
        # ä¸ mock è®¤è¯ä¿¡æ¯ï¼Œä½¿ç”¨çœŸå®çš„ç¯å¢ƒå˜é‡
        # ç¡®ä¿æœåŠ¡å™¨ä¼šæ­£å¸¸åˆå§‹åŒ–è®¤è¯

        # åˆ›å»ºä¸€ä¸ª httpx transportï¼ŒæŒ‡å‘æˆ‘ä»¬çš„ FastAPI åº”ç”¨
        from httpx import ASGITransport

        transport = ASGITransport(app=app)

        # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼Œä½¿ç”¨è‡ªå®šä¹‰ transport
        client = openai.AsyncOpenAI(
            api_key=API_KEY,
            base_url="http://test/v1",
            http_client=AsyncClient(transport=transport, base_url="http://test")
        )
        yield client
        await client.close()

    @pytest.mark.asyncio
    async def test_list_models_with_openai_client(self, openai_client):
        """æµ‹è¯•ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯åˆ—å‡ºæ¨¡å‹"""
        models = await openai_client.models.list()
        print("=== List Models ===")
        print(models)

        assert models.object == "list"
        assert len(models.data) > 0

        # éªŒè¯æ¨¡å‹æ ¼å¼
        for model in models.data:
            assert hasattr(model, 'id')
            assert hasattr(model, 'object')
            assert hasattr(model, 'created')
            assert hasattr(model, 'owned_by')
            assert model.object == "model"

    @pytest.mark.asyncio
    async def test_chat_completion_with_openai_client(self, openai_client):
        """æµ‹è¯•ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è¿›è¡ŒèŠå¤©å®Œæˆ"""
        response = await openai_client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {"role": "user", "content": "Hello, how are you?"}
            ],
            max_tokens=50
        )
        print("=== Chat Completion Response ===")
        print(response)

        # éªŒè¯å“åº”æ ¼å¼
        assert hasattr(response, 'id')
        assert hasattr(response, 'object')
        assert hasattr(response, 'created')
        assert hasattr(response, 'model')
        assert hasattr(response, 'choices')
        assert hasattr(response, 'usage')

        assert response.object == "chat.completion"
        assert response.model == "deepseek-reasoner"
        assert len(response.choices) > 0

        # éªŒè¯é€‰æ‹©æ ¼å¼
        choice = response.choices[0]
        assert hasattr(choice, 'index')
        assert hasattr(choice, 'message')
        assert hasattr(choice, 'finish_reason')

        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        message = choice.message
        assert hasattr(message, 'role')
        assert hasattr(message, 'content')
        assert message.role == "assistant"

    @pytest.mark.asyncio
    async def test_streaming_chat_completion_with_openai_client(self, openai_client):
        """æµ‹è¯•ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è¿›è¡Œæµå¼èŠå¤©å®Œæˆ"""
        stream = await openai_client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {"role": "user", "content": "Count from 1 to 3"}
            ],
            max_tokens=30,
            stream=True
        )

        chunks = []
        async for chunk in stream:
            chunks.append(chunk)

            # éªŒè¯æ¯ä¸ª chunk çš„æ ¼å¼
            assert hasattr(chunk, 'id')
            assert hasattr(chunk, 'object')
            assert hasattr(chunk, 'created')
            assert hasattr(chunk, 'model')
            assert hasattr(chunk, 'choices')

            assert chunk.object == "chat.completion.chunk"
            assert chunk.model == "deepseek-reasoner"
            assert len(chunk.choices) > 0

            # éªŒè¯é€‰æ‹©æ ¼å¼
            choice = chunk.choices[0]
            assert hasattr(choice, 'index')
            assert hasattr(choice, 'delta')
            assert hasattr(choice, 'finish_reason')

        # åº”è¯¥æ”¶åˆ°å¤šä¸ª chunks
        assert len(chunks) > 1

        # æœ€åä¸€ä¸ª chunk åº”è¯¥æœ‰ finish_reason
        assert chunks[-1].choices[0].finish_reason is not None

    @pytest.mark.asyncio
    async def test_function_calling_with_openai_client(self, openai_client):
        """æµ‹è¯•ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è¿›è¡Œå‡½æ•°è°ƒç”¨"""
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_current_weather",
                    "description": "Get the current weather in a given location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA"
                            }
                        },
                        "required": ["location"]
                    }
                }
            }
        ]

        response = await openai_client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {"role": "user", "content": "What's the weather like in Boston?"}
            ],
            tools=tools,
            max_tokens=100
        )

        # éªŒè¯å“åº”æ ¼å¼
        assert response.object == "chat.completion"
        assert len(response.choices) > 0

        # éªŒè¯æ¶ˆæ¯å¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨æˆ–æ™®é€šå†…å®¹
        message = response.choices[0].message
        assert hasattr(message, 'role')
        assert message.role == "assistant"

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼ŒéªŒè¯æ ¼å¼
        if hasattr(message, 'tool_calls') and message.tool_calls:
            for tool_call in message.tool_calls:
                assert hasattr(tool_call, 'id')
                assert hasattr(tool_call, 'type')
                assert hasattr(tool_call, 'function')
                assert tool_call.type == "function"

                function = tool_call.function
                assert hasattr(function, 'name')
                assert hasattr(function, 'arguments')

    @pytest.mark.asyncio
    async def test_invalid_api_key(self):
        """æµ‹è¯•æ— æ•ˆçš„ API key"""
        from httpx import ASGITransport

        transport = ASGITransport(app=app)
        client = openai.AsyncOpenAI(
            api_key="invalid-key",
            base_url="http://test/v1",
            http_client=AsyncClient(transport=transport, base_url="http://test")
        )

        with pytest.raises(openai.AuthenticationError):
            await client.models.list()

        await client.close()

    @pytest.mark.asyncio
    async def test_missing_api_key(self):
        """æµ‹è¯•ç¼ºå°‘ API key"""
        from httpx import ASGITransport

        transport = ASGITransport(app=app)
        client = openai.AsyncOpenAI(
            api_key="",  # ç©ºçš„ API key
            base_url="http://test/v1",
            http_client=AsyncClient(transport=transport, base_url="http://test")
        )

        with pytest.raises(openai.AuthenticationError):
            await client.models.list()

        await client.close()

    @pytest.mark.asyncio
    async def test_multiple_models_support(self, openai_client):
        """æµ‹è¯•å¤šä¸ªæ¨¡å‹çš„æ”¯æŒ"""
        models_to_test = ["deepseek-reasoner", "gpt-4o-mini", "claude", "deepseek-chat"]

        for model in models_to_test:
            response = await openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "Say hello"}
                ],
                max_tokens=20
            )

            assert response.object == "chat.completion"
            assert response.model == model
            assert len(response.choices) > 0

if __name__ == "__main__":
    # è¿è¡Œä¸€ä¸ªç®€å•çš„æµ‹è¯•ç¤ºä¾‹
    async def simple_test():
        from httpx import ASGITransport

        # ä¸ mock è®¤è¯ä¿¡æ¯ï¼Œä½¿ç”¨çœŸå®çš„ç¯å¢ƒå˜é‡

        transport = ASGITransport(app=app)
        client = openai.AsyncOpenAI(
            api_key=API_KEY,
            base_url="http://test/v1",
            http_client=AsyncClient(transport=transport, base_url="http://test")
        )

        print("ğŸ§ª æµ‹è¯•æ¨¡å‹åˆ—è¡¨...")
        models = await client.models.list()
        print(f"âœ… æˆåŠŸè·å– {len(models.data)} ä¸ªæ¨¡å‹")

        print("ğŸ§ª æµ‹è¯•èŠå¤©å®Œæˆ...")
        try:
            response = await client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[{"role": "user", "content": "Hello!"}],
                max_tokens=20
            )
            print(f"âœ… æˆåŠŸè·å–å›å¤: {response.choices[0].message.content[:50]}...")
        except Exception as e:
            if "500" in str(e) or "timeout" in str(e).lower():
                print(f"âœ… æ­£ç¡®å¤„ç†äº†åç«¯é”™è¯¯: {str(e)[:50]}...")
            else:
                print(f"âŒ æ„å¤–é”™è¯¯: {e}")

        await client.close()

    print("ğŸš€ è¿è¡Œç®€å•æµ‹è¯•...")
    asyncio.run(simple_test())
    print("âœ… ç®€å•æµ‹è¯•å®Œæˆ!")
